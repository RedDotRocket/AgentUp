# AgentUp Configuration - Standard Template
# AI-powered agent with MCP integration - recommended for most users

# Agent Information
agent:
  name: {{ project_name }}
  description: {{ description }}
  version: 0.1.0

# Core skills configuration
skills:
  - skill_id: ai_assistant
    name: AI Assistant
    description: AI-powered assistant for various tasks
    tags: [ai, assistant, helper]  # Tags for use within the @ai_function decorator
    # No keywords or patterns defined, only available via AI routing
    input_mode: text
    output_mode: text
    priority: 100

# Registry skills section - for skills installed from AgentUp Skills Registry
registry_skills: []

# Plugin configuration
plugins: []

# Security configuration
security:
  enabled: false
  type: "api_key"
  api_key:
    header_name: "X-API-Key"
    location: "header"  # Options: header, query, cookie
    # The below is randomly generated, and not hardcoded, please change if relevant
    keys:
      - "{{ generate_api_key() }}"


# AI configuration
{% if ai_provider_config %}
ai_provider:
    provider: {{ ai_provider_config.provider }}
{% if ai_provider_config.provider == 'openai' %}
    api_key: ${OPENAI_API_KEY}
    model: {{ ai_provider_config.model | default('gpt-4o-mini') }}
{% elif ai_provider_config.provider == 'anthropic' %}
    api_key: ${ANTHROPIC_API_KEY}
    model: {{ ai_provider_config.model | default('claude-3-5-sonnet-20241022') }}
{% elif ai_provider_config.provider == 'ollama' %}
    model: {{ ai_provider_config.model | default('llama3') }}
    base_url: ${OLLAMA_BASE_URL:http://localhost:11434/v1}
{% endif %}
    temperature: 0.7
    max_tokens: 1000
    top_p: 1.0
{% endif %}

# AI system prompt and configuration
ai:
  enabled: true
  system_prompt: |
    You are an AI agent created by AgentUp with access to specific functions and skills.
    
    Your role:
    - Understand user requests naturally and conversationally
    - Use the appropriate functions when needed to complete tasks
    - Provide helpful, accurate responses based on function results
    - Maintain context across conversations
    - Handle various content types when supported by plugins
    
    When users ask for something:
    1. If you have a relevant function, call it with appropriate parameters
    2. If multiple functions are needed, call them in logical order
    3. Synthesize the results into a natural, helpful response
    4. If no function is needed, respond conversationally
    
    Always be helpful, accurate, and maintain a friendly professional tone.

# External services configuration (non-LLM services)
services:
  valkey:
    type: cache
    config:
      url: '${VALKEY_URL:valkey://localhost:6379}'
      db: 1                    # Use DB 1 for cache
      max_connections: 10

# Model Context Protocol configuration
mcp:
  enabled: true
  client:
    enabled: true
    servers:
      - name: filesystem
        command: npx
        args: ['-y', '@modelcontextprotocol/server-filesystem', '/tmp']
        env: {}
  server:
    enabled: true
    name: {{ project_name }}-mcp-server
    expose_handlers: true
    expose_resources: [agent_status, agent_capabilities]

# middleware configuration
middleware:
  - name: logged
    params:
      log_level: 20  # INFO level
  - name: timed
    params: {}
{% if has_middleware and 'cache' in feature_config.get('middleware', []) %}
  - name: cached
    params:
      ttl: 300  # 5 minutes
{% endif %}
{% if has_middleware and 'rate_limit' in feature_config.get('middleware', []) %}
  - name: rate_limited
    params:
      requests_per_minute: 60
{% endif %}
{% if has_middleware and 'retry' in feature_config.get('middleware', []) %}
  - name: retryable
    params:
      max_retries: 3
      backoff_factor: 2
{% endif %}

# Push notifications configuration
push_notifications:
  enabled: true
  backend: memory             # Simple memory backend for standard template
  validate_urls: true         # Enable webhook URL validation


# State management configuration
state_management:
  enabled: true
  backend: valkey          # Use valkey backend for better performance
  ttl: 3600                # 1 hour expiration
  config:
    url: '${VALKEY_URL:valkey://localhost:6379}'
    key_prefix: "agentup:state:"

# Legacy state configuration (still supported)
state:
  backend: valkey
  ttl: 3600  # 1 hour expiration
  url: "valkey://localhost:6379"
  key_prefix: "agentup:state:"

# Logging configuration
logging:
  enabled: true
  level: "INFO"
  format: "text"  # Use "json" for production environments
  
  # Console output settings
  console:
    enabled: true
    colors: true
  
  # File logging (disabled by default for standard template)
  file:
    enabled: false
    path: "logs/{{ project_name_snake }}.log"
    rotation: "100 MB"
    retention: "1 week"
  
  # Advanced features
  correlation_id: true        # Add correlation IDs to requests
  request_logging: true       # Log HTTP requests/responses
  
  # Module-specific log levels
  modules:
    uvicorn: "INFO"
    httpx: "WARNING"
    
  # Uvicorn integration
  uvicorn:
    access_log: true
    disable_default_handlers: true
    use_colors: true

